{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import HTMLParser as html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib\n",
    "#import nltk\n",
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieves story headlines from NYT website\n",
    "response = requests.get(\"http://www.nytimes.com/\")\n",
    "parser = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "top_story = []\n",
    "\n",
    "top_link = []\n",
    "\n",
    "top_stories = parser.select('div.a-column')[0]\n",
    "top_headings = top_stories.select('h2.story-heading')\n",
    "\n",
    "for story in top_headings:\n",
    "    try:\n",
    "        top_story.append(story.get_text())\n",
    "        pattern = re.compile(\"http.+\\.html\")\n",
    "        head = re.search(pattern, str(story)).group(0)\n",
    "        head.replace('\"','')\n",
    "        top_link.append(head)\n",
    "    except:\n",
    "        top_link.append(None)\n",
    "    \n",
    "top_df = pd.DataFrame({'headline':top_story,'link':top_link})\n",
    "top_df = top_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#story input is an index 0 to n; where n is the number of headlines in top_df\n",
    "def NYT(story):\n",
    "    \n",
    "    #retrieve story link and html from that link\n",
    "    story_link = top_df.link[story]\n",
    "\n",
    "    response = requests.get(story_link)\n",
    "    parser = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    #parse article to grab text and title\n",
    "            \n",
    "    title = top_df.headline[story]\n",
    "    \n",
    "    article = []\n",
    "\n",
    "    for para in parser.select('p.story-body-text'):\n",
    "        article.append(para.get_text().replace(u'.\\u201d', u'\\u201d.'))\n",
    " \n",
    "    #create article string\n",
    "    s = ' '\n",
    "    article1 = s.join(article).strip()\n",
    "    \n",
    "    #TFIDF for full text article\n",
    "    stopwords = ['mr','chili','pic','sale','amazon','co','wire','a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "    vect = TfidfVectorizer(stop_words = stopwords, ngram_range = (2,2))\n",
    "    tfidf = pd.DataFrame(vect.fit_transform([article1]).toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "    top_tokens = pd.DataFrame(pd.DataFrame(tfidf.sum().sort_values(ascending=False))).reset_index().rename(columns={'index':'token',0:'freq'})\n",
    "\n",
    "    #processing article text into sentences\n",
    "    \n",
    "    tokens = TextBlob(article1).sentences\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in tokens:\n",
    "        sentences.append(str(sentence).decode('utf-8'))\n",
    "        \n",
    "    #split article by sentences and put in df        \n",
    "    article_df = pd.DataFrame({'sentence':sentences})\n",
    "\n",
    "    #function for scoring each sentence for importance\n",
    "    def sentence_score(string):\n",
    "        score = 0.0\n",
    "        string = TextBlob(string)\n",
    "        for word in string.split(' '):\n",
    "            word = word.lower()\n",
    "            if word in list(top_tokens[top_tokens.freq > top_tokens.freq.mean()]['token']):\n",
    "                score = score + top_tokens[top_tokens.token == word]['freq'].iloc[0]\n",
    "            else:\n",
    "                score = score + 0\n",
    "        return score\n",
    "    \n",
    "    def sentence_score3(string):\n",
    "        score = []\n",
    "        sentence = string.split(' ')\n",
    "        for token in list(top_tokens['token']):\n",
    "            for word in token.lower().split(' '):\n",
    "                word = word.lower()\n",
    "                if word in sentence:\n",
    "                    score.append(top_tokens[top_tokens.token == token]['freq'].iloc[0])\n",
    "                else:\n",
    "                    score.append(0)\n",
    "        return pd.Series(score).mean()*len(sentence)\n",
    "\n",
    "    #score each sentence\n",
    "    article_df['score'] = article_df.sentence.apply(sentence_score3)\n",
    "\n",
    "    #segment article by intro, body, and end\n",
    "    position = []\n",
    "\n",
    "    article_len = len(article_df) \n",
    "    round(article_len*.10)\n",
    "\n",
    "    intro = [1]*int(round(article_len*.15))\n",
    "    end = [3]*int(round(article_len*.33))\n",
    "    body = [2]*int(round(article_len - (len(intro)+len(end))))\n",
    "\n",
    "    position = intro + body + end\n",
    "\n",
    "    #add position column in article df\n",
    "    article_df['position'] = position\n",
    "\n",
    "    #construct list of sentences for summary using most important sentences from each position\n",
    "    try:\n",
    "        summary = [article_df[article_df.position == 1].sort_values(by='score', ascending = False).sentence.iloc[0], \n",
    "                   article_df[article_df.position == 2].sort_values(by='score', ascending = False).sentence.iloc[0], \n",
    "                   article_df[article_df.position == 3].sort_values(by='score', ascending = False).sentence.iloc[0]]\n",
    "        s = ' '\n",
    "        summarized = s.join(summary)\n",
    "        \n",
    "    except:\n",
    "        summarized = ' '.join(article_df.sentence)\n",
    "\n",
    "    #join the sentences together and fix punctuation\n",
    "    pattern = re.compile('\\[[0-9]+\\]')\n",
    "    summarized = re.sub(pattern,'', summarized)\n",
    "    \n",
    "    if summarized[-1] != '.':\n",
    "        summarized = summarized + '.'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    summarized = summarized.replace(' ,',', ').replace(' .','.').replace(' \\'s', '\\'s').replace('( ','(').replace(' )',')').replace(' \"','').replace(' :',':').replace(' ;',';')\n",
    "\n",
    "    \n",
    "    \n",
    "    #compression\n",
    "    summ_len = len(summarized)\n",
    "    long_len = len(article1)\n",
    "    compression = (1 - float(summ_len)/long_len)*100\n",
    "    \n",
    "    \n",
    "    #return article_link, summarized\n",
    "    print title + '\\n' + story_link + '\\n\\n' + summarized + '\\n\\n' + 'Compression Stats:\\nOriginal Article Length: ' + str(long_len) + ' characters\\nSummary Length: ' + str(summ_len) + ' characters\\n' + str(compression) + '% compressed'\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senators and Spy Chief Rebut Trump on Russia Findings\n",
      "http://www.nytimes.com/2017/01/05/us/politics/taking-aim-at-trump-leaders-strongly-affirm-findings-on-russian-hacking.html\n",
      "\n",
      "WASHINGTON — Rebuffing efforts by President-elect Donald J. Trump to cast doubt on Russian interference in the presidential election, top intelligence officials and senators from both parties on Thursday issued a forceful affirmation of the findings. Though Mr. Clapper and most Republican senators were careful to avoid antagonizing the president-elect directly, the hearing spoke to the searing rift Mr. Trump has threatened to create between the incoming administration and the intelligence officials tasked with informing it. Ms. McCaskill said there would be “howls from the Republican side of the aisle” if a Democrat had spoken about intelligence officials as Mr. Trump has.\n",
      "\n",
      "Compression Stats:\n",
      "Original Article Length: 6678 characters\n",
      "Summary Length: 681 characters\n",
      "89.8023360288% compressed\n"
     ]
    }
   ],
   "source": [
    "NYT(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
