{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "#import HTMLParser as html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import urllib\n",
    "#import nltk\n",
    "from textblob import TextBlob, Word\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieves story headlines from NYT website\n",
    "def get_headlines():\n",
    "    response = requests.get(\"http://www.nytimes.com/\")\n",
    "    parser = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    top_story = []\n",
    "    secondary_story = []\n",
    "\n",
    "    top_link = []\n",
    "    secondary_link = []\n",
    "\n",
    "    top_stories = parser.select('div.a-column')[0]\n",
    "    top_headings = top_stories.select('h2.story-heading')\n",
    "\n",
    "    for story in top_headings:\n",
    "        try:\n",
    "            top_story.append(story.get_text())\n",
    "            pattern = re.compile(\"http.+\\.html\")\n",
    "            head = re.search(pattern, str(story)).group(0)\n",
    "            head.replace('\"','')\n",
    "            top_link.append(head)\n",
    "        except:\n",
    "            top_link.append(None)\n",
    "\n",
    "    top_df = pd.DataFrame({'headline':top_story,'link':top_link})\n",
    "    top_df.dropna(inplace=True)\n",
    "    top_df.reset_index(inplace=True)\n",
    "    top_df=top_df[['headline','link']]\n",
    "    \n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Defending Tumult of Presidency, Calls to End ‘...</td>\n",
       "      <td>https://www.nytimes.com/2017/02/28/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Widow of Fallen SEAL Becomes a Face of Bravery</td>\n",
       "      <td>https://www.nytimes.com/2017/03/01/us/politics...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Defending Tumult of Presidency, Calls to End ‘...   \n",
       "1    Widow of Fallen SEAL Becomes a Face of Bravery    \n",
       "\n",
       "                                                link  \n",
       "0  https://www.nytimes.com/2017/02/28/us/politics...  \n",
       "1  https://www.nytimes.com/2017/03/01/us/politics...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df = get_headlines()\n",
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_story(story):\n",
    "    story_link = top_df.link[story]\n",
    "\n",
    "    response = requests.get(story_link)\n",
    "    parser = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    #parse article to grab text, link, and title\n",
    "    article_link = story_link\n",
    "\n",
    "    title = top_df.headline[story]\n",
    "\n",
    "    article = []\n",
    "\n",
    "    for para in parser.select('p.story-body-text'):\n",
    "        article.append(para.get_text().replace(u'.\\u201d', u'\\u201d.')\n",
    "                                      .replace(u'!\\u201d', u'\\u201d!')\n",
    "                                      .replace(u'?\\u201d', u'\\u201d?')\n",
    "                                      .replace(u'\\u25a0',''))\n",
    "\n",
    "    #create string holding entire article text\n",
    "    article = ' '.join(article).strip()\n",
    "    return article, title, article_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#story input is an index 0 to n; where n is the number of headlines in top_df\n",
    "def summ(text):\n",
    "\n",
    "    #tokes out any words with excess periods ('.') that may incorrectly break up sentences\n",
    "    def no_abbv(article):\n",
    "        article_list = []\n",
    "        for word in article.split(' '):\n",
    "            if word.count('.') > 1 and word[-1] == '\"':\n",
    "                article_list.append(word.replace('.',''))\n",
    "            if word.count('.') > 1:\n",
    "                article_list.append(word.replace('.',''))    \n",
    "            else:\n",
    "                article_list.append(word)\n",
    "\n",
    "        return ' '.join(article_list)\n",
    "\n",
    "    #cleans text to alpha characters only\n",
    "    def clean(text_list):\n",
    "        cleaned = []\n",
    "        for word in text_list:\n",
    "            try:\n",
    "                cleaned.append(re.sub('[^A-Za-z]+','', word))\n",
    "            except:\n",
    "                cleaned.append(word)\n",
    "\n",
    "        try:\n",
    "            return list(cleaned.remove(''))\n",
    "\n",
    "        except:\n",
    "            return cleaned\n",
    "\n",
    "    #function to stem words\n",
    "    def stemmer(text):\n",
    "        # initialize stemmer\n",
    "        stem1 = SnowballStemmer('english')\n",
    "        return clean([stem1.stem(word) for word in TextBlob(text).lower().words])\n",
    "\n",
    "    # standardize feature scores\n",
    "    def scaler(feature):\n",
    "        score = feature.values.reshape(-1,1)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(score)\n",
    "        return scaler.transform(score)\n",
    "\n",
    "    article = no_abbv(text)\n",
    "\n",
    "    #TFIDF for full text article\n",
    "    stopwords = stemmer(' '.join(['mr','mrs','ms','said','a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'happen', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']))\n",
    "\n",
    "    def tf_idf(text, ngram_range):\n",
    "        vect = TfidfVectorizer(stop_words = stopwords, ngram_range=ngram_range)\n",
    "        tfidf = pd.DataFrame(vect.fit_transform([text]).toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "        top_tokens = pd.DataFrame(pd.DataFrame(tfidf.sum().sort_values(ascending=False))).reset_index().rename(columns={'index':'token',0:'freq'})\n",
    "        return top_tokens\n",
    "\n",
    "    top_tokens = tf_idf(article,ngram_range = (1,1))\n",
    "\n",
    "    #processing article text into sentences\n",
    "    def make_article_df(article):\n",
    "        tokens = TextBlob(article).sentences\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in tokens:\n",
    "            sentences.append(str(sentence).decode('utf-8'))\n",
    "\n",
    "        #split article by sentences and put in df        \n",
    "        return pd.DataFrame({'sentence':sentences})\n",
    "\n",
    "    article_df = make_article_df(article)\n",
    "\n",
    "    #sentence importance scoring algorithm\n",
    "    def sentence_score3(string_list,token_df):\n",
    "        score = []\n",
    "        #string_list = clean(string_list)\n",
    "        for word in stopwords:\n",
    "            if word in string_list[:]:\n",
    "                string_list.remove(word)\n",
    "        for index,row in token_df.head(5).iterrows():\n",
    "            for token1 in list(set(list(row.stem))):\n",
    "                for word in string_list:\n",
    "                    if token1 in string_list:\n",
    "                        score.append(row.freq)\n",
    "                    else:\n",
    "                        score.append(0)                                                 \n",
    "\n",
    "        #for word in stem_title:\n",
    "        #    if word in string_list:\n",
    "        #        score.append(1)\n",
    "        #    else:\n",
    "        #        score.append(0)\n",
    "\n",
    "        #return pd.Series(score).mean()#*len(sentence)\n",
    "        try:\n",
    "            return pd.Series(score).sum()/len(string_list)\n",
    "            #return pd.Series(score).sum()/len(row.stem)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    #jaccard score for sentences for final sentence scoring\n",
    "    def jaccard(summary,df):\n",
    "        jaccard_scores = []\n",
    "        summary = ' '.join(summary)\n",
    "\n",
    "        #summary = summary + ' ' + title\n",
    "\n",
    "        first = list(set(stemmer(summary)))\n",
    "        for word in stopwords:\n",
    "            if word in first[:]:\n",
    "                first.remove(word)\n",
    "        first = [x for x in first if x != '']\n",
    "        first = set(first)\n",
    "\n",
    "        for sentence in df.stem:\n",
    "            second = list(set(sentence))\n",
    "            for word in stopwords:\n",
    "                if word in second[:]:\n",
    "                    second.remove(word)\n",
    "            second = [x for x in second if x != '']\n",
    "            second = set(second)\n",
    "            jaccard_scores.append((len(first & second) / float(len(first | second))))\n",
    "\n",
    "        return jaccard_scores\n",
    "\n",
    "    def jaccard_keys(summary,df):\n",
    "        jaccard_terms = []\n",
    "        summary = ' '.join(summary)\n",
    "\n",
    "        #summary = summary + ' ' + title\n",
    "\n",
    "        first = list(set(stemmer(summary)))\n",
    "        for word in stopwords:\n",
    "            if word in first[:]:\n",
    "                first.remove(word)\n",
    "        first = [x for x in first if x != '']\n",
    "        first = set(first)\n",
    "\n",
    "        for sentence in df.stem:\n",
    "            second = list(set(sentence))\n",
    "            for word in stopwords:\n",
    "                if word in second[:]:\n",
    "                    second.remove(word)\n",
    "            second = [x for x in second if x != '']\n",
    "            second = set(second)\n",
    "            jaccard_terms.append((first & second))\n",
    "\n",
    "        return jaccard_terms\n",
    "\n",
    "\n",
    "\n",
    "    #stem_title = clean(stemmer(title)) \n",
    "\n",
    "    #stem column\n",
    "    article_df['stem'] = article_df.sentence.apply(stemmer)\n",
    "\n",
    "    top_tokens['stem'] = top_tokens.token.apply(stemmer)\n",
    "\n",
    "    #score each sentence\n",
    "    article_df['score'] = article_df.stem.apply(sentence_score3,token_df = top_tokens)\n",
    "\n",
    "    #segment article by intro, body, and end\n",
    "    position = []\n",
    "\n",
    "    article_len = len(article_df) \n",
    "    round(article_len*.10)\n",
    "\n",
    "    intro = [1]*int(round(article_len*.15))\n",
    "    end = [3]*int(round(article_len*.33))\n",
    "    body = [2]*int(round(article_len - (len(intro)+len(end))))\n",
    "\n",
    "    position = intro + body + end\n",
    "\n",
    "    #add position column in article df\n",
    "    article_df['position'] = position\n",
    "\n",
    "    #enumerate each sentence\n",
    "    article_df.reset_index(inplace=True)\n",
    "    article_df=article_df.rename(columns={'index':'num'})\n",
    "\n",
    "    #construct list of sentences for summary using most important sentences from each position\n",
    "    summary = [article_df.sort_values(by='score', ascending = False).sentence.iloc[0], \n",
    "               article_df.sort_values(by='score', ascending = False).sentence.iloc[1]]\n",
    "\n",
    "    #tfidf for first 2 lines picked\n",
    "    summ_tokens = tf_idf(' '.join(summary),ngram_range = (2,2))\n",
    "    summ_tokens['stem'] = summ_tokens.token.apply(stemmer)\n",
    "\n",
    "    #creates summ_df and summary\n",
    "    def make_summ():\n",
    "\n",
    "        #first = article_df.sort_values(by='score', ascending = False).sentence.iloc[0]\n",
    "        #second = article_df.sort_values(by='score', ascending = False).sentence.iloc[1]\n",
    "\n",
    "\n",
    "        #summ_df1 = article_df[article_df.sentence != first]\n",
    "        #summ_df2 = article_df[article_df.sentence != second]\n",
    "\n",
    "        #summ_df = pd.merge(summ_df1[['sentence','num','score']],summ_df2[['sentence','num','score']])\n",
    "        \n",
    "        summ_df = article_df[['sentence','num','score']]\n",
    "\n",
    "        summ_df['stem'] = summ_df.sentence.apply(stemmer)\n",
    "\n",
    "        return summ_df\n",
    "\n",
    "    pos3_df = make_summ()\n",
    "    pos3_df['score'] = pos3_df.stem.apply(sentence_score3, token_df = summ_tokens)\n",
    "\n",
    "    pos3_df['jaccard_score'] = jaccard(summary,pos3_df)\n",
    "    article_df['jaccard_score'] = jaccard(summary,article_df)\n",
    "    pos3_df['jaccard_keys'] = jaccard_keys(summary,pos3_df)\n",
    "    article_df['jaccard_keys'] = jaccard_keys(summary,article_df)\n",
    "    \n",
    "    article_df['score_scaled'] = scaler(article_df['score'])\n",
    "    article_df['jaccard_scaled'] = scaler(article_df['jaccard_score'])\n",
    "    pos3_df['score_scaled'] = scaler(pos3_df['score'])\n",
    "    pos3_df['jaccard_scaled'] = scaler(pos3_df['jaccard_score'])\n",
    "    \n",
    "    avg = []\n",
    "    for index,row in pos3_df.iterrows():\n",
    "        #avg.append(np.mean([row.score,row.jaccard_score]))\n",
    "        avg.append((row.score_scaled+row.jaccard_scaled)/2)\n",
    "    pos3_df['avg'] = avg\n",
    "\n",
    "    \n",
    "    pos3_df['avg_scaled'] = scaler(pos3_df['avg'])\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    for key,row in pos3_df.iterrows():\n",
    "        for i in summary:\n",
    "            if row.sentence == i:\n",
    "                pos3_df.loc[count,'avg_scaled'] = 0\n",
    "            else:\n",
    "                pass\n",
    "        count = count+1\n",
    "\n",
    "        \n",
    "\n",
    "    #put summary sentences in order !!!!! needs work !!!!!    \n",
    "    def order_summ():\n",
    "        summary = pd.DataFrame([article_df[article_df.num == pos3_df[pos3_df.avg_scaled < 5].sort_values(by='avg_scaled', ascending = False).num.iloc[0]][['sentence','num','score']].iloc[0],\n",
    "                                article_df.sort_values(by='score', ascending = False)[['sentence','num','score']].iloc[0], \n",
    "                                article_df.sort_values(by='score', ascending = False)[['sentence','num','score']].iloc[1]])\n",
    "\n",
    "        if str(list(summary.sort_values(by='num').sentence)[0][0:4]).lower() == 'but ':\n",
    "            summ_list = [summary.sort_values(by='score',ascending=False).sentence.iloc[1],\n",
    "                         summary.sort_values(by='score',ascending=False).sentence.iloc[2],\n",
    "                         summary.sort_values(by='score',ascending=False).sentence.iloc[0]]\n",
    "\n",
    "        else:\n",
    "            summ_list = list(summary.sort_values(by='num').sentence)\n",
    "\n",
    "        sentence_list = list(summary.sort_values(by='num').num)\n",
    "\n",
    "        return (summ_list, sentence_list)\n",
    "\n",
    "\n",
    "    summarized, sentence_list = order_summ()\n",
    "    summarized = ' '.join(summarized)\n",
    "\n",
    "\n",
    "    if summarized[-1] != '.':\n",
    "        summarized = summarized + '.'\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    summarized = summarized.replace(' ,',', ').replace(' .','.').replace(' \\'s', '\\'s').replace('( ','(').replace(' )',')').replace(' \"','').replace(' :',':').replace(' ;',';')\n",
    "\n",
    "    #compression\n",
    "    summ_len = len(summarized)\n",
    "    long_len = len(article)\n",
    "    reduction = (1 - float(summ_len)/long_len)*100\n",
    "    reduction_stat = 'Reduction Stats: [ Summary Length: ' + str(summ_len) + ' chars | Original Article Length: ' + str(long_len) + ' chars | ' + \"%.2f\" % reduction + '% redux ] Sentences Used: ' + str(sentence_list)\n",
    "\n",
    "    slack_summ = ['*' + title + '*','>'+summarized,reduction_stat]\n",
    "    slack_summ = '\\n'.join(slack_summ).encode('utf-8')\n",
    "\n",
    "    return {'article_df':article_df, \n",
    "            'top_tokens':top_tokens, \n",
    "            'pos3_df':pos3_df, \n",
    "            'summ_tokens':summ_tokens, \n",
    "            'summary':summarized,\n",
    "            'reduction':reduction_stat,\n",
    "            'sentence_list':sentence_list,\n",
    "            'slack_summ':slack_summ}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article, title, article_link = get_story(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Ryan\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "pkg = summ(article)\n",
    "article_df = pkg['article_df']\n",
    "top_tokens = pkg['top_tokens']\n",
    "pos3_df = pkg['pos3_df']\n",
    "summ_tokens = pkg['summ_tokens']\n",
    "summarized = pkg['summary']\n",
    "reduction = pkg['reduction']\n",
    "sentence_list = pkg['sentence_list']\n",
    "slack_summ = pkg['slack_summ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Defending Tumult of Presidency, Calls to End ‘Trivial Fights’*\n",
      ">Only hours before his address, Mr. Trump had broken from his tough immigration stance in remarks at the White House, suggesting that legal status be granted to millions of undocumented immigrants who have not committed serious crimes. Although Mr. Trump’s presidency has been defined by executive orders and pronouncements, his speech appeared to be an attempt to open a new phase and reflected his need for cooperation from Congress. Even as he held out the possibility of legal status for millions of undocumented immigrants, Melania Trump, the first lady, was hosting the families of victims of violent crime by such immigrants — a way of highlighting the president’s belief that immigrants who lack legal status pose a grave threat to Americans and should be feared and removed, not embraced.\n",
      "Reduction Stats: [ Summary Length: 796 chars | Original Article Length: 9199 chars | 91.35% redux ] Sentences Used: [3, 24, 53]\n"
     ]
    }
   ],
   "source": [
    "#print title + '\\n' + article_link + '\\n\\n' + summarized.decode('utf-8') + '\\n\\n' + reduction + '\\nSentences used: ' + str(sentence_list)\n",
    "\n",
    "print slack_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
