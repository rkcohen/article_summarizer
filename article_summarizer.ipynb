{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import HTMLParser as html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib\n",
    "#import nltk\n",
    "from textblob import TextBlob, Word\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieves story headlines from NYT website\n",
    "def get_headlines():\n",
    "    response = requests.get(\"http://www.nytimes.com/\")\n",
    "    parser = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    top_story = []\n",
    "    secondary_story = []\n",
    "\n",
    "    top_link = []\n",
    "    secondary_link = []\n",
    "\n",
    "    top_stories = parser.select('div.a-column')[0]\n",
    "    top_headings = top_stories.select('h2.story-heading')\n",
    "\n",
    "    for story in top_headings:\n",
    "        try:\n",
    "            top_story.append(story.get_text())\n",
    "            pattern = re.compile(\"http.+\\.html\")\n",
    "            head = re.search(pattern, str(story)).group(0)\n",
    "            head.replace('\"','')\n",
    "            top_link.append(head)\n",
    "        except:\n",
    "            top_link.append(None)\n",
    "\n",
    "    top_df = pd.DataFrame({'headline':top_story,'link':top_link})\n",
    "    top_df = top_df.dropna()\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_df = get_headlines()\n",
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_story(story):\n",
    "    story_link = top_df.link[story]\n",
    "\n",
    "    response = requests.get(story_link)\n",
    "    parser = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    #parse article to grab text, link, and title\n",
    "    article_link = story_link\n",
    "\n",
    "    title = top_df.headline[story]\n",
    "\n",
    "    article = []\n",
    "\n",
    "    for para in parser.select('p.story-body-text'):\n",
    "        article.append(para.get_text().replace(u'.\\u201d', u'\\u201d.')\n",
    "                                      .replace(u'!\\u201d', u'\\u201d!')\n",
    "                                      .replace(u'?\\u201d', u'\\u201d?')\n",
    "                                      .replace(u'\\u25a0',''))\n",
    "\n",
    "    #create string holding entire article text\n",
    "    article = ' '.join(article).strip()\n",
    "    return article, title, article_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#story input is an index 0 to n; where n is the number of headlines in top_df\n",
    "def summ(text):\n",
    "    \n",
    "    #tokes out any words with excess periods ('.') that may incorrectly break up sentences\n",
    "    def no_abbv(article):\n",
    "        article_list = []\n",
    "        for word in article.split(' '):\n",
    "            if word.count('.') > 1 and word[-1] == '\"':\n",
    "                article_list.append(word.replace('.',''))\n",
    "            if word.count('.') > 1:\n",
    "                article_list.append(word.replace('.',''))    \n",
    "            else:\n",
    "                article_list.append(word)\n",
    "\n",
    "        return ' '.join(article_list)\n",
    "    \n",
    "    #cleans text to alpha characters only\n",
    "    def clean(text_list):\n",
    "        cleaned = []\n",
    "        for word in text_list:\n",
    "            try:\n",
    "                cleaned.append(re.sub('[^A-Za-z]+','', word))\n",
    "            except:\n",
    "                cleaned.append(word)\n",
    "                \n",
    "        try:\n",
    "            return list(cleaned.remove(''))\n",
    "        \n",
    "        except:\n",
    "            return cleaned\n",
    "       \n",
    "    #function to stem words\n",
    "    def stemmer(text):\n",
    "        # initialize stemmer\n",
    "        stem1 = SnowballStemmer('english')\n",
    "        return clean([stem1.stem(word) for word in TextBlob(text).lower().words])\n",
    "    \n",
    "    article = no_abbv(text)\n",
    "    \n",
    "    #TFIDF for full text article\n",
    "    stopwords = stemmer(' '.join(['mr','mrs','ms','said','a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'happen', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']))\n",
    "    \n",
    "    def tf_idf(text):\n",
    "        vect = TfidfVectorizer(stop_words = stopwords, ngram_range = (2,2))\n",
    "        tfidf = pd.DataFrame(vect.fit_transform([text]).toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "        top_tokens = pd.DataFrame(pd.DataFrame(tfidf.sum().sort_values(ascending=False))).reset_index().rename(columns={'index':'token',0:'freq'})\n",
    "        return top_tokens\n",
    "    \n",
    "    top_tokens = tf_idf(article)\n",
    "\n",
    "    #processing article text into sentences\n",
    "    def make_article_df(article):\n",
    "        tokens = TextBlob(article).sentences\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for sentence in tokens:\n",
    "            sentences.append(str(sentence).decode('utf-8'))\n",
    "\n",
    "        #split article by sentences and put in df        \n",
    "        return pd.DataFrame({'sentence':sentences})\n",
    "    \n",
    "    article_df = make_article_df(article)\n",
    "    \n",
    "    #sentence importance scoring algorithm\n",
    "    def sentence_score3(string_list,token_df):\n",
    "        score = []\n",
    "        #string_list = clean(string_list)\n",
    "        for word in stopwords:\n",
    "            if word in string_list[:]:\n",
    "                string_list.remove(word)\n",
    "        for index,row in token_df.head(5).iterrows():\n",
    "            for token1 in list(set(list(row.stem))):\n",
    "                for word in string_list:\n",
    "                    if token1 in string_list:\n",
    "                        score.append(row.freq)\n",
    "                    else:\n",
    "                        score.append(0)                                                 \n",
    "        \n",
    "        #for word in stem_title:\n",
    "        #    if word in string_list:\n",
    "        #        score.append(1)\n",
    "        #    else:\n",
    "        #        score.append(0)\n",
    "\n",
    "        #return pd.Series(score).mean()#*len(sentence)\n",
    "        try:\n",
    "            return pd.Series(score).sum()/len(string_list)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    #jaccard score for sentences for final sentence scoring\n",
    "    def jaccard(summary,df):\n",
    "        #stopwords = ['mr','mrs','ms','said','a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "        jaccard_scores = []\n",
    "        summary = ' '.join(summary)\n",
    "        \n",
    "        #summary = summary + ' ' + title\n",
    "        \n",
    "        first = list(set(stemmer(summary)))\n",
    "        for word in stopwords:\n",
    "            if word in first[:]:\n",
    "                first.remove(word)\n",
    "        first = set(first)\n",
    "\n",
    "        for sentence in df.stem:\n",
    "            second = list(set(sentence))\n",
    "            for word in stopwords:\n",
    "                if word in second[:]:\n",
    "                    second.remove(word)\n",
    "            second = set(second)\n",
    "            jaccard_scores.append((len(first & second) / float(len(first | second))))\n",
    "\n",
    "        return jaccard_scores\n",
    "    \n",
    "    def jaccard_keys(summary,df):\n",
    "        #stopwords = ['mr','mrs','ms','said','a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "        jaccard_terms = []\n",
    "        summary = ' '.join(summary)\n",
    "        \n",
    "        #summary = summary + ' ' + title\n",
    "        \n",
    "        first = list(set(stemmer(summary)))\n",
    "        for word in stopwords:\n",
    "            if word in first[:]:\n",
    "                first.remove(word)\n",
    "        first = set(first)\n",
    "\n",
    "        for sentence in df.stem:\n",
    "            second = list(set(sentence))\n",
    "            for word in stopwords:\n",
    "                if word in second[:]:\n",
    "                    second.remove(word)\n",
    "            second = set(second)\n",
    "            jaccard_terms.append((first & second))\n",
    "\n",
    "        return jaccard_terms\n",
    "\n",
    "    \n",
    "    \n",
    "    #stem_title = clean(stemmer(title)) \n",
    "                        \n",
    "    #stem column\n",
    "    article_df['stem'] = article_df.sentence.apply(stemmer)\n",
    "    \n",
    "    top_tokens['stem'] = top_tokens.token.apply(stemmer)\n",
    "    \n",
    "    #score each sentence\n",
    "    article_df['score'] = article_df.stem.apply(sentence_score3,token_df = top_tokens)\n",
    "\n",
    "    #segment article by intro, body, and end\n",
    "    position = []\n",
    "\n",
    "    article_len = len(article_df) \n",
    "    round(article_len*.10)\n",
    "\n",
    "    intro = [1]*int(round(article_len*.15))\n",
    "    end = [3]*int(round(article_len*.33))\n",
    "    body = [2]*int(round(article_len - (len(intro)+len(end))))\n",
    "\n",
    "    position = intro + body + end\n",
    "\n",
    "    #add position column in article df\n",
    "    article_df['position'] = position\n",
    "    \n",
    "    #enumerate each sentence\n",
    "    article_df.reset_index(inplace=True)\n",
    "    article_df=article_df.rename(columns={'index':'num'})\n",
    "\n",
    "    #construct list of sentences for summary using most important sentences from each position\n",
    "    summary = [article_df.sort_values(by='score', ascending = False).sentence.iloc[0], \n",
    "               article_df.sort_values(by='score', ascending = False).sentence.iloc[1]]\n",
    "    \n",
    "    #tfidf for first 2 lines picked\n",
    "    summ_tokens = tf_idf(' '.join(summary))\n",
    "    summ_tokens['stem'] = summ_tokens.token.apply(stemmer)\n",
    "    \n",
    "    #creates summ_df and summary\n",
    "    def make_summ():\n",
    "        \n",
    "        first = article_df.sort_values(by='score', ascending = False).sentence.iloc[0]\n",
    "        second = article_df.sort_values(by='score', ascending = False).sentence.iloc[1]\n",
    "        \n",
    "        \n",
    "        summ_df1 = article_df[article_df.sentence != first]\n",
    "        summ_df2 = article_df[article_df.sentence != second]\n",
    "    \n",
    "        summ_df = pd.merge(summ_df1[['sentence','num','score']],summ_df2[['sentence','num','score']])\n",
    "        \n",
    "        summ_df['stem'] = summ_df.sentence.apply(stemmer)\n",
    "        \n",
    "        return summ_df\n",
    "    \n",
    "    pos3_df = make_summ()\n",
    "    pos3_df['score'] = pos3_df.stem.apply(sentence_score3, token_df = summ_tokens)\n",
    "    \n",
    "    pos3_df['jaccard_score'] = jaccard(summary,pos3_df)\n",
    "    article_df['jaccard_score'] = jaccard(summary,article_df)\n",
    "    pos3_df['jaccard_keys'] = jaccard_keys(summary,pos3_df)\n",
    "    article_df['jaccard_keys'] = jaccard_keys(summary,article_df)\n",
    "    \n",
    "    avg = []\n",
    "    for index,row in pos3_df.iterrows():\n",
    "        #avg.append(np.mean([row.score,row.jaccard_score]))\n",
    "        avg.append(row.score*row.jaccard_score)\n",
    "    pos3_df['avg'] = avg\n",
    "        \n",
    "    #put summary sentences in order !!!!! needs work !!!!!    \n",
    "    def order_summ():\n",
    "        summary = pd.DataFrame([article_df[article_df.num == pos3_df.sort_values(by='avg', ascending = False).num.iloc[0]][['sentence','num','score']].iloc[0],\n",
    "                                article_df.sort_values(by='score', ascending = False)[['sentence','num','score']].iloc[0], \n",
    "                                article_df.sort_values(by='score', ascending = False)[['sentence','num','score']].iloc[1]])\n",
    "        \n",
    "        if str(list(summary.sort_values(by='num').sentence)[0][0:4]).lower() == 'but ':\n",
    "            summ_list = [summary.sort_values(by='score',ascending=False).sentence.iloc[1],\n",
    "                         summary.sort_values(by='score',ascending=False).sentence.iloc[2],\n",
    "                         summary.sort_values(by='score',ascending=False).sentence.iloc[0]]\n",
    "            \n",
    "        else:\n",
    "            summ_list = list(summary.sort_values(by='num').sentence)\n",
    "        \n",
    "        sentence_list = list(summary.sort_values(by='num').num)\n",
    "        \n",
    "        return (summ_list, sentence_list)\n",
    "    \n",
    "    \n",
    "    summarized, sentence_list = order_summ()\n",
    "    summarized = ' '.join(summarized)\n",
    "    \n",
    "    \n",
    "    if summarized[-1] != '.':\n",
    "        summarized = summarized + '.'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    summarized = summarized.replace(' ,',', ').replace(' .','.').replace(' \\'s', '\\'s').replace('( ','(').replace(' )',')').replace(' \"','').replace(' :',':').replace(' ;',';')\n",
    "\n",
    "    \n",
    "    \n",
    "    #compression\n",
    "    summ_len = len(summarized)\n",
    "    long_len = len(article)\n",
    "    compression = (1 - float(summ_len)/long_len)*100\n",
    "    compression_stat = 'Compression Stats: [ Summary Length: ' + str(summ_len) + ' characters | Original Article Length: ' + str(long_len) + ' characters | ' + str(compression) + '% compressed ]'\n",
    "    \n",
    "    return {'article_df':article_df, \n",
    "            'top_tokens':top_tokens, \n",
    "            'pos3_df':pos3_df, \n",
    "            'summ_tokens':summ_tokens, \n",
    "            'summary':summarized,\n",
    "            'compression':compression_stat,\n",
    "            'sentence_list':sentence_list}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article, title, article_link = get_story(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkg = summ(article)\n",
    "article_df = pkg['article_df']\n",
    "top_tokens = pkg['top_tokens']\n",
    "pos3_df = pkg['pos3_df']\n",
    "summ_tokens = pkg['summ_tokens']\n",
    "summarized = pkg['summary']\n",
    "compression = pkg['compression']\n",
    "sentence_list = pkg['sentence_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print title + '\\n' + article_link + '\\n\\n' + summarized + '\\n\\n' + compression + '\\nSentences used: ' + str(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_df.score.plot(x = article_df.num, kind='line',color='g',linewidth=1.5)\n",
    "article_df.jaccard_score.plot(x = article_df.num, kind='line',color='r',linewidth=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pos3_df.jaccard_score.plot(x = pos3_df.num, kind='line',color='b',linewidth=1,ls = '--')\n",
    "pos3_df.score.plot(x = pos3_df.num, kind='line',color='g',linewidth=1,ls = '--')\n",
    "pos3_df.avg.plot(x = pos3_df.num, kind='line',color='r',linewidth=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_tokens.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summ_tokens.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
